{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велИчайшим усилеем выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с величайшим усилием выбравшись из потока убегающих людей кутузов со свитой уменьшившейся вдвое поехал на звуки выстрелов русских орудий\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# cчитываем слова из файла\n",
    "with open(\"07_nlp/data/litw-win.txt\", \"r\") as f:\n",
    "    words = [line.split()[1] for line in f]\n",
    "\n",
    "# исправление опечаток\n",
    "def correct_word(word, words):\n",
    "    if word in words:  # если слово в words, значит исправлять не надо, просто возвращаем слово\n",
    "        return word\n",
    "    else:\n",
    "        # находим ближайшее слово по расстоянию Левенштейна\n",
    "        min_distance = float('inf')\n",
    "        corrected_word = None\n",
    "        for w in words:\n",
    "            distance = Levenshtein.distance(word, w)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                corrected_word = w\n",
    "        return corrected_word\n",
    "\n",
    "corrected_sentence = \" \".join([correct_word(word, words) for word in text.split()])\n",
    "\n",
    "print(corrected_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "стемминг удаляет суффиксы и префиксы из слов\n",
    "\n",
    "лемматизация приводит слова к их основной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['с', 'величайшим', 'усилием', 'выбравшись', 'из', 'потока', 'убегающих', 'людей', 'кутузов', 'со', 'свитой', 'уменьшившейся', 'вдвое', 'поехал', 'на', 'звуки', 'выстрелов', 'русских', 'орудий']\n"
     ]
    }
   ],
   "source": [
    "#Разбиение текста на слова:\n",
    "import nltk\n",
    "\n",
    "words = nltk.word_tokenize(corrected_sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 2 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1]]\n",
      " \n",
      "[[1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1\n",
      "  0]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0\n",
      "  0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Токенизировать текст на предложения на русском языке\n",
    "texti = sent_tokenize(text, language=\"russian\")\n",
    "\n",
    "# Создать корпус из токенизированных предложений\n",
    "corpus = [i for i in texti]\n",
    "\n",
    "# Создать экземпляр CountVectorizer\n",
    "vectorizer1 = CountVectorizer()\n",
    "\n",
    "# Создать экземпляр CountVectorizer с анализатором уровня слова и диапазоном n-грамм (2, 2)\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "\n",
    "# Преобразовать корпус в матрицу признаков с помощью CountVectorizer\n",
    "X = vectorizer1.fit_transform(corpus)\n",
    "\n",
    "# Получить имена признаков из CountVectorizer\n",
    "vectorizer1.get_feature_names_out()\n",
    "\n",
    "# Вывести матрицу признаков\n",
    "print(X.toarray())\n",
    "print(\" \")\n",
    "\n",
    "# Преобразовать корпус в матрицу признаков с помощью CountVectorizer\n",
    "X = vectorizer2.fit_transform(corpus)\n",
    "\n",
    "# Получить имена признаков из CountVectorizer\n",
    "vectorizer2.get_feature_names_out()\n",
    "\n",
    "# Вывести матрицу признаков\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go  it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sister in law made these for us at a family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>this is a traditional fresh plum cake  thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>i ve heard of the  cookies by design  company ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "0             george s at the cove  black bean soup   \n",
       "1                healthy for them  yogurt popsicles   \n",
       "2                      i can t believe it s spinach   \n",
       "3                              italian  gut busters   \n",
       "4          love is in the air  beef fondue   sauces   \n",
       "...                                             ...   \n",
       "29995  zurie s holey rustic olive and cheddar bread   \n",
       "29996          zwetschgenkuchen  bavarian plum cake   \n",
       "29997   zwiebelkuchen   southwest german onion cake   \n",
       "29998                                   zydeco soup   \n",
       "29999        cookies by design   cookies on a stick   \n",
       "\n",
       "                               preprocessed_descriptions  \n",
       "0      an original recipe created by chef scott meska...  \n",
       "1      my children and their friends ask for my homem...  \n",
       "2                these were so go  it surprised even me   \n",
       "3      my sister in law made these for us at a family...  \n",
       "4      i think a fondue is a very romantic casual din...  \n",
       "...                                                  ...  \n",
       "29995  this is based on a french recipe but i changed...  \n",
       "29996  this is a traditional fresh plum cake  thought...  \n",
       "29997  this is a traditional late summer early fall s...  \n",
       "29998  this is a delicious soup that i originally fou...  \n",
       "29999  i ve heard of the  cookies by design  company ...  \n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = '07_nlp/preprocessed_descriptions.csv'\n",
    "fil = pd.read_csv(f)\n",
    "fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов: 24485\n",
      "Первые 10 уникальных слов: ['domesticgoddess', 'athens', 'ruined', 'overpowrer', 'patient', 'strayed', 'lynched', 'restrained', 'bakeries', 'nane']\n"
     ]
    }
   ],
   "source": [
    "file_path = '07_nlp/preprocessed_descriptions.csv'  # Укажите правильный путь к вашему файлу\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "descriptions = data['preprocessed_descriptions'].astype(str)\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for description in descriptions:\n",
    "    tokens = word_tokenize(description)\n",
    "    unique_words.update(tokens)\n",
    "\n",
    "print(f'Количество уникальных слов: {len(unique_words)}')\n",
    "print(f'Первые 10 уникальных слов: {list(unique_words)[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scared -> airtight: 8\n",
      "effortlessly -> twisted: 10\n",
      "river -> confit: 6\n",
      "corpse -> 429436: 6\n",
      "derink -> riddled: 7\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import Levenshtein\n",
    "\n",
    "# Сгенерировать 5 пар случайных слов\n",
    "pairs = [(random.choice(list(unique_words)), random.choice(list(unique_words))) for i in range(5)]\n",
    "\n",
    "# Посчитать расстояние редактирования для каждой пары\n",
    "distances = [Levenshtein.distance(pair[0], pair[1]) for pair in pairs]\n",
    "\n",
    "# Вывести пары слов и расстояния редактирования\n",
    "for pair, distance in zip(pairs, distances):\n",
    "    print(f\"{pair[0]} -> {pair[1]}: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['athens', 'ruined', 'nane', '14g', '148498']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def find_k_nearest_neighbors(word, words, k):\n",
    "    # Посчитать расстояния Левенштейна между заданным словом и всеми словами из списка\n",
    "    distances = [Levenshtein.distance(word, w) for w in words]\n",
    "\n",
    "    # Отсортировать слова по расстоянию Левенштейна\n",
    "    sorted_words = sorted(zip(distances, words), key=lambda x: x[0])\n",
    "\n",
    "    # Вернуть k ближайших соседей\n",
    "    return [w for d, w in sorted_words[:5]]\n",
    "\n",
    "find_k_nearest_neighbors('солнце', list(unique_words), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nasty\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nasty\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Убедитесь, что вы скачали необходимые ресурсы для nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    stemmed_word  normalized_word\n",
      "word                                             \n",
      "domesticgoddess  domesticgoddess  domesticgoddess\n",
      "athens                     athen           athens\n",
      "ruined                      ruin           ruined\n",
      "overpowrer              overpowr       overpowrer\n",
      "patient                  patient          patient\n",
      "...                          ...              ...\n",
      "000                          000              000\n",
      "ram                          ram              ram\n",
      "squashed                  squash         squashed\n",
      "jello                      jello            jello\n",
      "emerges                    emerg          emerges\n",
      "\n",
      "[24485 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Инициализация стеммера и лемматизатора\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Стемминг и лемматизация\n",
    "stemmed_words = [stemmer.stem(word) for word in list(unique_words)]\n",
    "normalized_words = [lemmatizer.lemmatize(word) for word in list(unique_words)]  # English words will be lemmatized correctly\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'word': list(unique_words),\n",
    "    'stemmed_word': stemmed_words,\n",
    "    'normalized_word': normalized_words\n",
    "})\n",
    "\n",
    "# Установка столбца 'word' в качестве индекса\n",
    "df.set_index('word', inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nasty\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество слов: 1104518\n",
      "Количество стоп-слов: 520201\n",
      "Доля стоп-слов: 47.10%\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for description in descriptions:\n",
    "    tokens = word_tokenize(description.lower())  # Преобразование в нижний регистр для унификации\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "total_words_count = len(all_words)\n",
    "print(f'Общее количество слов: {total_words_count}')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in all_words if word not in stop_words]\n",
    "\n",
    "# Подсчет количества стоп-слов\n",
    "stop_words_count = total_words_count - len(filtered_words)\n",
    "stop_words_fraction = stop_words_count / total_words_count\n",
    "\n",
    "print(f'Количество стоп-слов: {stop_words_count}')\n",
    "print(f'Доля стоп-слов: {stop_words_fraction:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 слов до удаления стоп-слов:\n",
      "[('the', 40413), ('a', 35131), ('and', 30585), ('i', 27945), ('this', 27181), ('to', 23598), ('it', 23300), ('is', 20306), ('of', 18405), ('for', 16023)]\n",
      "Топ-10 слов после удаления стоп-слов:\n",
      "[('recipe', 15198), ('make', 6438), ('time', 5287), ('use', 4652), ('great', 4522), ('like', 4276), ('easy', 4263), ('one', 4018), ('good', 3887), ('made', 3874)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Подсчет частотности слов\n",
    "original_counter = Counter(all_words)\n",
    "filtered_counter = Counter(filtered_words)\n",
    "\n",
    "# Топ-10 самых часто употребляемых слов до удаления стоп-слов\n",
    "top_10_original = original_counter.most_common(10)\n",
    "print('Топ-10 слов до удаления стоп-слов:')\n",
    "print(top_10_original)\n",
    "\n",
    "# Топ-10 самых часто употребляемых слов после удаления стоп-слов\n",
    "top_10_filtered = filtered_counter.most_common(10)\n",
    "print('Топ-10 слов после удаления стоп-слов:')\n",
    "print(top_10_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранные случайные рецепты:\n",
      "---------------------------------------------------------------------------------\n",
      "20952    great for the kids  hamburger meat makes the c...\n",
      "3648     picked up this recipe from womansday  the perf...\n",
      "819                              from the magnolia bakery \n",
      "24299                poor man s saffron rice   still good \n",
      "9012                        requires 1 hour chilling time \n",
      "Name: preprocessed_descriptions, dtype: object\n",
      "---------------------------------------------------------------------------------\n",
      "Числовые векторы для выбранных рецептов:\n",
      "---------------------------------------------------------------------------------\n",
      "             12        16  absolutely   amazing       and    bakery      book  \\\n",
      "20952  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.188525   \n",
      "3648   0.116932  0.116932    0.116932  0.116932  0.350797  0.000000  0.000000   \n",
      "819    0.000000  0.000000    0.000000  0.000000  0.000000  0.587521  0.000000   \n",
      "24299  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9012   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        chicken  chilling  complete  ...  together  toppings        up  \\\n",
      "20952  0.000000       0.0  0.188525  ...  0.188525  0.188525  0.000000   \n",
      "3648   0.116932       0.0  0.000000  ...  0.000000  0.000000  0.116932   \n",
      "819    0.000000       0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "24299  0.000000       0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "9012   0.000000       0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "           used  vegetables        we      with  womansday      your     yummy  \n",
      "20952  0.000000    0.000000  0.000000  0.188525   0.000000  0.000000  0.188525  \n",
      "3648   0.116932    0.116932  0.350797  0.000000   0.116932  0.116932  0.000000  \n",
      "819    0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "24299  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "9012   0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Установка зерна для воспроизводимости\n",
    "random_indices = random.sample(range(len(descriptions)), 5)\n",
    "random_descriptions = descriptions.iloc[random_indices]\n",
    "\n",
    "print(\"Выбранные случайные рецепты:\")\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(random_descriptions)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Преобразование описаний в числовые векторы\n",
    "tfidf_matrix = vectorizer.fit_transform(random_descriptions)\n",
    "\n",
    "# Преобразование матрицы в DataFrame для наглядности\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=random_indices, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(\"Числовые векторы для выбранных рецептов:\")\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(tfidf_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Числовые векторы для выбранных рецептов:\n",
      "---------------------------------------------------------------------------------\n",
      "             12        16  absolutely   amazing       and    bakery      book  \\\n",
      "20952  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.188525   \n",
      "3648   0.116932  0.116932    0.116932  0.116932  0.350797  0.000000  0.000000   \n",
      "819    0.000000  0.000000    0.000000  0.000000  0.000000  0.587521  0.000000   \n",
      "24299  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9012   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        chicken  chilling  complete  ...  together  toppings        up  \\\n",
      "20952  0.000000       0.0  0.188525  ...  0.188525  0.188525  0.000000   \n",
      "3648   0.116932       0.0  0.000000  ...  0.000000  0.000000  0.116932   \n",
      "819    0.000000       0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "24299  0.000000       0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "9012   0.000000       0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "           used  vegetables        we      with  womansday      your     yummy  \n",
      "20952  0.000000    0.000000  0.000000  0.188525   0.000000  0.000000  0.188525  \n",
      "3648   0.116932    0.116932  0.350797  0.000000   0.116932  0.116932  0.000000  \n",
      "819    0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "24299  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "9012   0.000000    0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 70 columns]\n",
      "---------------------------------------------------------------------------------\n",
      "Таблица косинусных расстояний:\n",
      "---------------------------------------------------------------------------------\n",
      "preprocessed_descriptions                          great for the kids  hamburger meat makes the crust that is then filled with yummy pizza toppings  quick to put together  taken from  the complete family cook book    \\\n",
      "preprocessed_descriptions                                                                                                                                                                                                 \n",
      "great for the kids  hamburger meat makes the cr...                                                0.0                                                                                                                     \n",
      "picked up this recipe from womansday  the perfe...                                           0.886778                                                                                                                     \n",
      "from the magnolia bakery                                                                     0.801286                                                                                                                     \n",
      "poor man s saffron rice   still good                                                              1.0                                                                                                                     \n",
      "requires 1 hour chilling time                                                                     1.0                                                                                                                     \n",
      "\n",
      "preprocessed_descriptions                          picked up this recipe from womansday  the perfect mix of curry chicken and fresh vegetables made easy in your sow cooker  we made this and it tasted great the day we made it and absolutely amazing the next day    we used 12 16 drummettes instead of drumsticks   \\\n",
      "preprocessed_descriptions                                                                                                                                                                                                                                                                                                 \n",
      "great for the kids  hamburger meat makes the cr...                                           0.886778                                                                                                                                                                                                                     \n",
      "picked up this recipe from womansday  the perfe...                                                0.0                                                                                                                                                                                                                     \n",
      "from the magnolia bakery                                                                     0.876748                                                                                                                                                                                                                     \n",
      "poor man s saffron rice   still good                                                              1.0                                                                                                                                                                                                                     \n",
      "requires 1 hour chilling time                                                                     1.0                                                                                                                                                                                                                     \n",
      "\n",
      "preprocessed_descriptions                          from the magnolia bakery   \\\n",
      "preprocessed_descriptions                                                      \n",
      "great for the kids  hamburger meat makes the cr...                  0.801286   \n",
      "picked up this recipe from womansday  the perfe...                  0.876748   \n",
      "from the magnolia bakery                                                 0.0   \n",
      "poor man s saffron rice   still good                                     1.0   \n",
      "requires 1 hour chilling time                                            1.0   \n",
      "\n",
      "preprocessed_descriptions                          poor man s saffron rice   still good   \\\n",
      "preprocessed_descriptions                                                                  \n",
      "great for the kids  hamburger meat makes the cr...                                   1.0   \n",
      "picked up this recipe from womansday  the perfe...                                   1.0   \n",
      "from the magnolia bakery                                                             1.0   \n",
      "poor man s saffron rice   still good                                                 0.0   \n",
      "requires 1 hour chilling time                                                        1.0   \n",
      "\n",
      "preprocessed_descriptions                          requires 1 hour chilling time   \n",
      "preprocessed_descriptions                                                          \n",
      "great for the kids  hamburger meat makes the cr...                            1.0  \n",
      "picked up this recipe from womansday  the perfe...                            1.0  \n",
      "from the magnolia bakery                                                      1.0  \n",
      "poor man s saffron rice   still good                                          1.0  \n",
      "requires 1 hour chilling time                                                 0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Получение имен рецептов для использования в качестве меток\n",
    "recipe_names = data.iloc[random_indices]['preprocessed_descriptions']\n",
    "\n",
    "print(\"Числовые векторы для выбранных рецептов:\")\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(tfidf_df)\n",
    "\n",
    "# Инициализация DataFrame для хранения косинусных расстояний\n",
    "cosine_distances = pd.DataFrame(index=recipe_names, columns=recipe_names)\n",
    "\n",
    "# Вычисление косинусного расстояния между каждой парой рецептов\n",
    "for i in range(len(recipe_names)):\n",
    "    for j in range(len(recipe_names)):\n",
    "        if i == j:\n",
    "            cosine_distances.iloc[i, j] = 0.0\n",
    "        else:\n",
    "            cosine_distances.iloc[i, j] = cosine(tfidf_matrix[i].toarray()[0], tfidf_matrix[j].toarray()[0])\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(\"Таблица косинусных расстояний:\")\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(cosine_distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее похожие рецепты:\n",
      "great for the kids  hamburger meat makes the crust that is then filled with yummy pizza toppings  quick to put together  taken from  the complete family cook book  \n",
      " и \n",
      "from the magnolia bakery \n",
      " с косинусным расстоянием 0.8012862971727952\n"
     ]
    }
   ],
   "source": [
    "recipe_names = data.iloc[random_indices]['preprocessed_descriptions'].values\n",
    "\n",
    "min_distance = float('inf')\n",
    "min_pair = (None, None)\n",
    "for i in range(len(recipe_names)):\n",
    "    for j in range(i + 1, len(recipe_names)):\n",
    "        distance = cosine_distances.iloc[i, j]\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            min_pair = (recipe_names[i], recipe_names[j])\n",
    "\n",
    "print(f\"Наиболее похожие рецепты:\\n{min_pair[0]}\\n и \\n{min_pair[1]}\\n с косинусным расстоянием {min_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
